## 目标与原则
- 将“重复/冲突”判定结果视为临时态，仅用于本次导入的人工确认；不落库持久化。
- 简化导入链路：解析→检测→临时存储→用户确认（跳过/覆盖）→清理临时数据。
- 保留导入任务的汇总信息以便审计（文件名、计数、时间），但不保存逐条异常明细。

## 总体方案
- 临时存储载体：引入 `Redis` 作为导入会话的临时异常仓库（带 TTL），替代 `import_anomalies` 表。
- 键空间与TTL：
  - `import:batch:{batchId}:overview`（JSON，重复/冲突汇总）
  - `import:batch:{batchId}:anomalies`（哈希/列表，逐条冲突记录）
  - `TTL` 默认 24h（可配 `IMPORT_ANOMALY_TTL`），操作完成或批次关闭后立即删除。
- 流程：
  - 上传/CLI触发 → 解析 Excel → 检测重复/冲突 → 写入 Redis 临时键 → 返回摘要与批次 `batchId`。
  - 前端展示并执行“跳过/覆盖” → 后端根据选择执行 `upsert/skip` → 从 Redis 移除对应记录。
  - 批次完成：若无剩余异常，删除整个 `batchId` 键空间；导入任务状态置为 `completed`。

## 后端改造点（不立即修改，供确认）
- 移除异常落库：
  - 停用 `saveAnomalies()` 的持久化（backend/src/excel-import/excel-import.service.ts:572），改为写 Redis。
  - `getAnomalyOverview()`（backend/src/excel-import/excel-import.service.ts:242）改为从 Redis 组装返回。
  - `bulkResolveAnomalies()` 与 `resolveAnomaly()`（backend/src/excel-import/excel-import.service.ts:330, 363）改为：
    - 按 `batchId/anomalyId` 从 Redis 取候选；
    - `skip` 仅删除临时条目；
    - `overwrite` 执行 `sensor_data` 的 `upsert` 后删除临时条目；
    - 当该批次临时异常计数为 0 时，清理整个批次键并将任务状态更新为 `completed`。
- 导入任务表简化：
  - 保留 `import_tasks`（文件名、计数、消息），`status` 依据 Redis 中是否仍有异常。
  - 去除“异常明细”与“manualResolved”强耦合，改为从 Redis 动态计算。
- 归档文件与解析：保持现状，仅将 `fs.copyFileSync` 未来改为异步（backend/src/excel-import/excel-import.service.ts:676），此项可后续分批处理。
- 配置项：新增 `IMPORT_ANOMALY_TTL`、`IMPORT_ANOMALY_STORE=redis|memory`（开发/单机可选内存），以及 `IMPORT_SNAP_MINUTES` 控制时间对齐。

## 临时存储结构设计（Redis）
- `import:batch:{batchId}:overview`
  - 字段：`duplicates.pendingCount`、`duplicates.anomalyIds[]`、`duplicates.areaSummaries[]`、`conflicts[grouped]`。
- `import:batch:{batchId}:anomalies`
  - 列表/哈希元素：`{ anomalyId, areaName, timestamp, type, variants[] }`，与现有 DTO 对齐（frontend/types/import.ts）。
- 体量与压缩：仅保存“变体聚合”而非每条原始记录；平均每异常条 ~1KB，1万条约 10MB，可控。

## 前端改造点
- API 保持路径不变（兼容）：
  - `GET /imports/conflicts` 返回来自 Redis 的临时数据；
  - `POST /imports/conflicts/:id/resolve` 按 `skip/overwrite` 操作后端并实时减少计数；
  - `POST /imports/conflicts/bulk-resolve` 批量对重复数据操作；
- 新增显示：
  - 批次 `batchId` 与剩余处理数量、TTL 倒计时（防止临时数据过期）；
  - 当 TTL 过期/后端清理后，页面提示“本次导入冲突已清空”，无需历史明细。
- 类型兼容：沿用现有 `ImportConflictOverview` / `ResolvePayload`，仅追加可选 `batchId` 与 `ttlSeconds` 字段。

## API 契约调整（向下兼容）
- 响应中新增：`batchId`（上传与历史）、`ttlSeconds`（冲突概览）。
- 服务器端不再返回数据库异常主键，仅返回临时 `anomalyId`。
- 任务历史 `GET /imports/history` 维持统计字段，不含异常明细链接。

## 行为细节与边界条件
- 服务重启/Redis过期：临时异常消失 → 前端刷新后视为“需重新导入确认”；不自动重算旧异常。
- 并发处理：以 `anomalyId` 加分布式锁（Redis `SET NX` 短锁）避免重复覆写。
- 安全：保留 `DELETE /imports/reset` 仅开发可用；生产环境移除或加权限。

## 风险与缓解
- 丢失异常明细的审计能力：保留导入任务总量与最终落库结果即可；如需审计，改为“导入结果快照”文件存储（对象存储），而非逐条落库。
- Redis 体量管理：设置 TTL + 批次完成即清理；异常过多时分页加载与按区域分组键。

## 验证与测试
- 构造 10万条混合重复/冲突的 Excel，比较“临时存储方案”与“落库方案”的导入耗时与资源占用，目标减少异常写库 100%。
- 单测与集成：
  - 解析/分组/变体聚合；
  - Redis 读写与 TTL；
  - `skip/overwrite` 的数据正确性（`sensor_data` 幂等与覆盖）。

## 实施步骤
1. 接入 Redis 客户端与配置，封装 `AnomalyStoreService`（接口含 `saveOverview`, `saveAnomalies`, `getOverview`, `getAnomalies`, `resolveOne`, `resolveBulk`, `clearBatch`）。
2. 重构 `ExcelImportService` 的异常写入/读取路径为 `AnomalyStoreService`（参考点：backend/src/excel-import/excel-import.service.ts:242, 330, 363, 572）。
3. 控制器保留现有路由，实现从临时存储读取与处置（backend/src/excel-import/excel-import.controller.ts:61, 66）。
4. 调整 `ImportTask` 状态计算：依据临时存储有无剩余异常（backend/src/excel-import/excel-import.service.ts:544）。
5. 前端展示 TTL 与批次号，其他交互不变（frontend/src/pages/import/ImportPage.tsx）。
6. 增加配置与文档，开启特性开关；提供脚本清理旧 `import_anomalies` 历史数据。

## 里程碑
- M1：后端接入 Redis 并完成临时存储实现（含单元测试）。
- M2：服务端重构导入与处置逻辑；前端显示 TTL/批次号；端到端验证。
- M3：移除或冻结 `import_anomalies` 写库路径，保留任务汇总；压测与灰度上线。