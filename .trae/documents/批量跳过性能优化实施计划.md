## 修改点
- 前端：将 `handleBulkSkipDuplicates` 由逐条请求改为单次批量请求，进度模态框使用 `indeterminate` 模式并在完成后一次性刷新。
- 后端：优化 Redis 临时存储的 `bulkResolve`，使用 `MGET` 与 `pipeline(DEL/SREM)`，一次性处理所有 ID，并返回每项的 `areaName/timestamp/resolvedVariant`，避免后续再读。
- 后端服务：`ExcelImportService.bulkResolveAnomalies` 不再调用 `findById`，直接使用 `bulkResolve` 返回的 `areaName/timestamp` 写库（仅覆盖动作）。

## 验证
1. 启动后端（watch 模式自动编译），确保无 TS 错误。
2. 上传样例批次形成 5000+ 重复条目。
3. 点击“批量跳过”，确认用时从“分钟级”降至“秒级”；概览 `pendingCount=0`；数据库总数不变。
4. 执行“批量覆盖重复数据”，确认用时与结果正常。