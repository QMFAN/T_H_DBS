## 问题结论
- 慢的根因是前端在“批量跳过重复数据”时逐条发送请求（顺序 5700 次），而不是使用后端的批量接口。
- 证据：frontend/src/pages/import/ImportPage.tsx:145 处的 `handleBulkSkipDuplicates` 逐条 `await importService.skipConflict(anomalyIds[i])`，会造成大量 HTTP 往返。
- 后端已提供批量接口：`POST /imports/conflicts/bulk-resolve`（backend/src/excel-import/excel-import.controller.ts:66）。

## 优化方案
- 前端改造：
  - 将 `handleBulkSkipDuplicates` 改为调用 `importService.bulkResolveConflicts({ type: 'duplicate', action: 'skip', anomalyIds })`，与现有“批量覆盖重复数据”一致（frontend/src/pages/import/ImportPage.tsx:197 已用于覆盖）。
  - 进度模态框使用 `mode: 'indeterminate'` 并在完成后一次性刷新数据（与覆盖逻辑保持一致）。
- 后端 Redis 性能优化（可选增强）：
  - 在 `RedisAnomalyStoreService.bulkResolve` 中使用管道：先 `MGET` 所有 `anomaly` 键，过滤类型匹配，再使用 `pipeline` 批量 `DEL` + `SREM`，减少 5700×3 次往返。
  - 接口接受大 JSON：如需，调高 Nest/Express 的 `json` body 限制（默认足够，5700 UUID 约几十 KB）。

## 预期性能
- 前端单请求模式：一次 HTTP，后端批量执行；
- 内存存储：几百微秒到毫秒级；Redis 管道：典型 < 300ms（网络与实例性能相关）。
- 总体从“几分钟”降至“秒级”。

## 实施步骤
1. 修改前端 `handleBulkSkipDuplicates` 使用批量接口（frontend/src/pages/import/ImportPage.tsx:145）。
2. （可选）后端 Redis `bulkResolve` 改为管道执行（backend/src/excel-import/anomaly-store.redis.ts:83）。
3. 验证：
   - 构造 6000 条重复数据的批次；
   - 点击“批量跳过”，观察模态框与接口耗时；
   - 再次 `GET /imports/conflicts`，确认 `duplicates.pendingCount=0`；
   - 查询 MySQL，验证未执行写库操作（总记录数不变）。

## 成功标准
- 批量跳过 5700+ 条重复数据的总耗时显著下降（< 3s 为佳，网络环境允许）。
- 冲突概览与导入摘要同步更新，任务状态正确。